{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getuser # Libaray to copy things\n",
    "from pathlib import Path # Object oriented libary to deal with paths\n",
    "import os\n",
    "from tempfile import NamedTemporaryFile, TemporaryDirectory # Creating temporary Files/Dirs\n",
    "from subprocess import run, PIPE\n",
    "import sys\n",
    " \n",
    "import dask # Distributed data libary\n",
    "from dask_jobqueue import SLURMCluster # Setting up distributed memories via slurm\n",
    "from distributed import Client, progress, wait # Libaray to orchestrate distributed resources\n",
    "import xarray as xr # Libary to work with labeled n-dimensional data and dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some user specific variables\n",
    "scratch_dir = Path('/scratch') / getuser()[0] / getuser() # Define the users scratch dir\n",
    "# Create a temp directory where the output of distributed cluster will be written to, after this notebook\n",
    "# is closed the temp directory will be closed\n",
    "dask_tmp_dir = TemporaryDirectory(dir=scratch_dir, prefix='PostProc')\n",
    "cluster = SLURMCluster(memory='500GiB',\n",
    "                       cores=72,\n",
    "                       project='mh0731',\n",
    "                       walltime='1:00:00',\n",
    "                       queue='gpu',\n",
    "                       name='PostProc',\n",
    "                       scheduler_options={'dashboard_address': ':12435'},\n",
    "                       local_directory=dask_tmp_dir.name,\n",
    "                       job_extra=[f'-J PostProc', \n",
    "                                  f'-D {dask_tmp_dir.name}',\n",
    "                                  f'--begin=now',\n",
    "                                  f'--output={dask_tmp_dir.name}/LOG_cluster.%j.o',\n",
    "                                  f'--output={dask_tmp_dir.name}/LOG_cluster.%j.o'\n",
    "                                 ],\n",
    "                       interface='ib0')\n",
    "cluster.scale(jobs=2)\n",
    "dask_client = Client(cluster)\n",
    "dask_client.wait_for_workers(18)\n",
    "data_path = Path('/work/mh0287/k203123/GIT/icon-aes-dyw_albW/experiments/dpp0016/')\n",
    "glob_pattern_2d = 'atm2_2d_ml'\n",
    " \n",
    "# Collect all file names with pathlib's rglob and list compressions \n",
    "file_names = sorted([str(f) for f in data_path.rglob(f'*{glob_pattern_2d}*.nc')]) #[1:]\n",
    "dset = xr.open_mfdataset(file_names, combine='by_coords', parallel=True)\n",
    "var_names = ['pr']\n",
    "dset_subset = dset[var_names].persist()\n",
    "dset_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_mean = dset_subset.mean(dim='time').persist()\n",
    "field_mean = dset_subset.mean(dim='ncells').persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.utils import format_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_bytes(dset_subset.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_subset['pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_griddes(y_res, x_res, x_first=-180, y_first=-90):\n",
    "    \"\"\"Create a description for a regular global grid at given x, y resolution.\"\"\"\n",
    " \n",
    "    xsize = 360 / x_res\n",
    "    ysize = 180 / y_res\n",
    "    xfirst = -180 + x_res / 2\n",
    "    yfirst = -90 + x_res / 2\n",
    " \n",
    "    return f'''\n",
    "#\n",
    "# gridID 1\n",
    "#\n",
    "gridtype  = lonlat\n",
    "gridsize  = {int(xsize * ysize)}\n",
    "xsize     = {int(xsize)}\n",
    "ysize     = {int(ysize)}\n",
    "xname     = lon\n",
    "xlongname = \"longitude\"\n",
    "xunits    = \"degrees_east\"\n",
    "yname     = lat\n",
    "ylongname = \"latitude\"\n",
    "yunits    = \"degrees_north\"\n",
    "xfirst    = {xfirst}\n",
    "xinc      = {x_res}\n",
    "yfirst    = {yfirst}\n",
    "yinc      = {y_res}\n",
    " \n",
    " \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def gen_dis(dataset, xres, yres, gridfile):\n",
    "    '''Create a distance weights using cdo.'''\n",
    "    scratch_dir = Path('/scratch') / getuser()[0] / getuser() # Define the users scratch dir\n",
    "#     with TemporaryDirectory(dir=scratch_dir, prefix='Weights_') as td:\n",
    "    if True:\n",
    "        td = '/scratch/m/m300414/Weights_123'\n",
    "        in_file = Path(td) / 'in_file.nc'\n",
    "        weightfile = Path(td) / 'weight_file.nc'\n",
    "        griddes = Path(td) / 'griddes.txt'\n",
    "        with griddes.open('w') as f:\n",
    "            f.write(get_griddes(xres, yres))\n",
    "        dataset.to_netcdf(in_file, mode='w') # Write the file to a temorary netcdf file\n",
    "        cmd = ('cdo', '-O', f'gendis,{griddes}', f'-setgrid,{gridfile}', str(in_file), str(weightfile))\n",
    "        run_cmd(cmd)\n",
    "        df = xr.open_dataset(weightfile).load()\n",
    "        wait(df)\n",
    "        return df\n",
    " \n",
    "def run_cmd(cmd, path_extra=Path(sys.exec_prefix)/'bin'):\n",
    "    '''Run a bash command.'''\n",
    "    env_extra = os.environ.copy()\n",
    "    env_extra['PATH'] = str(path_extra) + ':' + env_extra['PATH']\n",
    "    status = run(cmd, check=False, stderr=PIPE, stdout=PIPE, env=env_extra)\n",
    "    if status.returncode != 0:\n",
    "        error = f'''{' '.join(cmd)}: {status.stderr.decode('utf-8')}'''\n",
    "        raise RuntimeError(f'{error}')\n",
    "    return status.stdout.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def remap(dataset, x_res, y_res, weights, gridfile):\n",
    "    \"\"\"Perform a weighted remapping.\n",
    " \n",
    "    Parameters\n",
    "    ==========\n",
    " \n",
    "    dataset : xarray.dataset\n",
    "        The dataset that will be regridded\n",
    "    griddes : Path, str\n",
    "        Path to the grid description file\n",
    "    weights : xarray.dataset\n",
    "        Distance weights\n",
    " \n",
    "    Returns\n",
    "    =======\n",
    "    xarray.dataset : Remapped dataset\n",
    "    \"\"\"\n",
    "    if isinstance(dataset, xr.DataArray):\n",
    "        # If a dataArray is given create a dataset\n",
    "        dataset = xr.Dataset(data_vars={dataset.name: dataset})\n",
    "    scratch_dir = Path('/scratch') / getuser()[0] / getuser() # Define the users scratch dir\n",
    "#     with TemporaryDirectory(dir=scratch_dir, prefix='Remap_') as td:\n",
    "    if True:\n",
    "        td = '/scratch/m/m300414/Remap_123'\n",
    "        infile = Path(td) / 'input_file.nc'\n",
    "        weightfile = Path(td) / 'weight_file.nc'\n",
    "        griddes = Path(td) / 'griddes.txt'\n",
    "        outfile = Path(td) / 'remaped_file.nc'\n",
    "        with griddes.open('w') as f:\n",
    "            f.write(get_griddes(x_res, y_res))\n",
    "        dataset.to_netcdf(infile, mode='w') # Write the file to a temorary netcdf file\n",
    "        weights.to_netcdf(weightfile, mode='w')\n",
    "        cmd = ('cdo', '-O', f'remap,{griddes},{weightfile}', f'-setgrid,{gridfile}',\n",
    "               str(infile), str(outfile))\n",
    "        run_cmd(cmd)\n",
    "        return xr.open_dataset(outfile).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file = '/pool/data/ICON/grids/public/mpim/0015/icon_grid_0015_R02B09_G.nc'\n",
    "weights_future = gen_dis(time_mean, 0.0225, 0.0225, grid_file)\n",
    "weights_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_subset['pr'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_futures = []\n",
    "# Process each variable in parallel.\n",
    "for snapshot in dset_subset['pr'][:2]:\n",
    "    remap_futures.append(remap(dset_subset['pr'].sel(time=snapshot.time.values.astype(str)), 0.0225, 0.0225, weights_future, grid_file))\n",
    "remap_futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_jobs = dask.persist(remap_futures)\n",
    "progress(remap_jobs, notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_remap = xr.concat(list(dask.compute(*remap_futures)), dim=dset_subset.time[:3])\n",
    "dset_remap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Save the time-series\n",
    "out_file = Path(scratch_dir) / 'dpp0016_precip.nc'\n",
    "dset_remap.to_netcdf(out_file, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 unstable (using the module python3/unstable)",
   "language": "python",
   "name": "python3_unstable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
